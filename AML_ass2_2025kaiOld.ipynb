{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Advanced Machine Learning - programming assignment 2\n",
    "\n",
    "*Due: Friday December 12th 23:59*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill in:**\n",
    "* name 1 (student id 1)\n",
    "* Kai ter Horst 3731200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further instructions:\n",
    "* Code quality is considered during the assessment (of ass 2 & 4). Make sure your code is properly commented. \n",
    "* Submit your code in BrightSpace only for ass 2 & 4.\n",
    "* When you submit the code, make sure to name the submitted file according to your and your collaborators last name (i.e. submitter_collaborator.ipynb).\n",
    "* Please notice that the grader most likely won't install additional packages. Try to stick with the standard library and the packages listed. \n",
    "* **Failure to follow these instructions can affect the assignment grade.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Reinforcement learning with function approximation\n",
    "\n",
    "In this assignment, you'll design your own agent to complete an episodic MDP task following the gymnasium framework. The agent will be looking at a small part of the UU logo, and have to decide which of the four compass directions (i.e. left, right, up, down) to move in. The learning task is to find the goal in the center as soon as possible.\n",
    "\n",
    "The learning objectives of this assignment are:\n",
    "- Implement two versions of the agent using Semi-gradient SARSA and Q-Learning algorithms with a linear approximation function,\n",
    "- Implement one agent using DQN algorithm (one deep RL method),\n",
    "- Demonstrate the difference between on-policy and off-policy RL methods,\n",
    "- Learn to integrate the approximation function with Tabular RL methods,\n",
    "- Play with the important parameters, e.g. discount factor $\\gamma$, stepsize $\\alpha$ and update frequency in DQN; and understand their influence on the learning procedure.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Let's start with setting up the enviroment.\n",
    "\n",
    "The following code defines various aspects of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from enum import IntEnum\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Constants defining the environment\n",
    "GOAL = (140, 120)\n",
    "CENTER = (132, 132)\n",
    "AVG_MOVEMENT_SIZE = 24\n",
    "ACCEPTABLE_DISTANCE_TO_GOAL = (AVG_MOVEMENT_SIZE // 2) + 1\n",
    "RADIUS = 72\n",
    "WINDOW_SIZE = 28\n",
    "TIME_LIMIT = 200\n",
    "TIMEOUT_REWARD = -100.0\n",
    "MOVE_REWARD = -1\n",
    "INVALID_MOVE_REWARD = -5\n",
    "\n",
    "\n",
    "# This is for type inference\n",
    "State = Tuple[int, int]\n",
    "\n",
    "\n",
    "# Action space\n",
    "class Actions(IntEnum):\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "\n",
    "# Boundaries\n",
    "class Boundary(IntEnum):\n",
    "    WEST = CENTER[0] - RADIUS\n",
    "    EAST = CENTER[0] + RADIUS\n",
    "    NORTH = CENTER[1] - RADIUS\n",
    "    SOUTH = CENTER[1] + RADIUS\n",
    "\n",
    "# Augmented boundarie, not used in this assignment\n",
    "class AugmentedArea(IntEnum):\n",
    "    WEST = Boundary.WEST - (WINDOW_SIZE // 2)\n",
    "    EAST = Boundary.EAST + (WINDOW_SIZE // 2)\n",
    "    NORTH = Boundary.NORTH - (WINDOW_SIZE // 2)\n",
    "    SOUTH = Boundary.SOUTH + (WINDOW_SIZE // 2)\n",
    "\n",
    "\n",
    "# Image\n",
    "ORIGINAL_IMAGE = plt.imread(\"UU_LOGO.png\")\n",
    "# Convert to one color channel (using only the red channel), with white background\n",
    "IMAGE = ORIGINAL_IMAGE[:, :, 0] * ORIGINAL_IMAGE[:, :, 3] + (1.0 - ORIGINAL_IMAGE[:, :, 3])\n",
    "\n",
    "\n",
    "# Get a \"camera view\" at the position indicated by state\n",
    "# Use reshape=True to format the output as a data point for the neural network\n",
    "def get_window(state: State, reshape=False) -> np.ndarray:\n",
    "    # When indexing the image as an array, switch the coordinates: im[state[1], state[0]]\n",
    "    window = IMAGE[(state[1] - 14):(state[1] + 14), (state[0] - 14):(state[0] + 14)]\n",
    "    if reshape:\n",
    "        return np.reshape(window, (1, 28, 28, 1))\n",
    "    return window\n",
    "\n",
    "\n",
    "# Is the state close enough to the goal to be considered a success?\n",
    "# There is a margin for error, so that the agent can't jump over the goal\n",
    "def is_goal_reached(state: State) -> bool:\n",
    "    return np.amax(np.abs(np.asarray(state) - np.asarray(GOAL))) <= ACCEPTABLE_DISTANCE_TO_GOAL\n",
    "\n",
    "\n",
    "# This is a helper function to render a run\n",
    "def updatefig(j, images, imgplot, text_act_plot, text_reward_plot):\n",
    "    # set the data in the axesimage object\n",
    "    img, time_point, from_state, to_state, act, current_reward = images[min(len(images), j)]\n",
    "    imgplot.set_data(img)\n",
    "    text_act_plot.set_text(f\"Time step: {time_point} - Action: {act}\\nState: {from_state} -> {to_state}\")\n",
    "    text_reward_plot.set_text(f\"Current total reward: {current_reward}\")\n",
    "    # return the artists set\n",
    "    return [imgplot, text_act_plot]\n",
    "\n",
    "\n",
    "# This will render a run of a full epoch\n",
    "# The function needs a list of tuples containing an image array, a State, the performed action\n",
    "def render_epoch(animation_data: List[Tuple[np.ndarray, State, Actions]], interval=100, blit=True, **kwargs):\n",
    "    if not len(animation_data):\n",
    "        return f\"No images in the list\"\n",
    "    fig, ax = plt.subplots()\n",
    "    imgplot = ax.imshow(np.zeros_like(animation_data[0][0]))\n",
    "    text_act_plot = ax.set_title(\"\", color=\"red\", fontweight=\"extra bold\", loc=\"left\")\n",
    "    text_reward_plot = ax.text(5, 255, \"\", color=\"red\", fontweight=\"extra bold\")\n",
    "    params = [animation_data, imgplot, text_act_plot, text_reward_plot]\n",
    "    ani = FuncAnimation(fig,\n",
    "                        updatefig,\n",
    "                        fargs=params,\n",
    "                        frames=len(animation_data),\n",
    "                        interval=interval,\n",
    "                        blit=blit,\n",
    "                        **kwargs)\n",
    "    animation = HTML(ani.to_jshtml())\n",
    "    plt.close()\n",
    "    return display.display(animation)\n",
    "\n",
    "# This function can be used to smooth obtained plots\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts) / box_pts\n",
    "    return np.convolve(y, box, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following 2 images show:\n",
    " * The original image, with a red dot marking the goal and a red rectangle marking the area where the center of agent must remain. A movement that would take the agent outside this rectangle, places him at the boundary instead. The blue rectangle represents an augmented area that is not necessary in the assignement but with which you can play.\n",
    " * What the agent sees if s/he is exactly at the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(IMAGE[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "# Plotting uses reversed y-axis now: larger y values are further down\n",
    "goal_container = plt.plot(GOAL[0], GOAL[1], 'rx', markersize=\"7\")\n",
    "legend2 = plt.legend(goal_container, [\"Goal\"], loc=3)\n",
    "\n",
    "plt.plot([Boundary.WEST, Boundary.WEST, Boundary.EAST, Boundary.EAST, Boundary.WEST],\n",
    "         [Boundary.NORTH, Boundary.SOUTH, Boundary.SOUTH, Boundary.NORTH, Boundary.NORTH],\n",
    "         'r-',\n",
    "         label=\"Movevable area\")\n",
    "plt.plot([AugmentedArea.WEST, AugmentedArea.WEST, AugmentedArea.EAST, AugmentedArea.EAST, AugmentedArea.WEST],\n",
    "         [AugmentedArea.NORTH, AugmentedArea.SOUTH, AugmentedArea.SOUTH, AugmentedArea.NORTH, AugmentedArea.NORTH],\n",
    "         'b-',\n",
    "         label=\"Viewable area\")\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().add_artist(legend2)\n",
    "plt.show()\n",
    "\n",
    "# window around goal\n",
    "img_container = plt.imshow(get_window(GOAL),\n",
    "                           cmap='gray',\n",
    "                           vmin=0,\n",
    "                           vmax=1.0,\n",
    "                           extent=(GOAL[0] - ACCEPTABLE_DISTANCE_TO_GOAL, GOAL[0] + ACCEPTABLE_DISTANCE_TO_GOAL,\n",
    "                                   GOAL[1] + ACCEPTABLE_DISTANCE_TO_GOAL, GOAL[1] - ACCEPTABLE_DISTANCE_TO_GOAL))\n",
    "plt.plot(GOAL[0], GOAL[1], 'ro', linewidth=1)\n",
    "plt.title(\"Acceptance area around goal\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class provides the functionality of tile encoding. The implementation can be used to define multiple tilings. The default code uses three tilings. You can play with different number of tilings, but please deliver the results with only one setting with multiple tilings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_tilings():\n",
    "    def __init__(self, number_of_grids = 1, offsets = np.array([[0.0, 0.0]]), bins_per_dim = (9, 9)) -> None:\n",
    "        # Low value for each dimension, for each grid / tile\n",
    "        lows = np.array([[Boundary.NORTH, Boundary.WEST]]*number_of_grids)\n",
    "        # High value for each dimension, for each grid / tile\n",
    "        highs = np.array([[Boundary.SOUTH, Boundary.EAST]]*number_of_grids)\n",
    "        # Number of discrete bin for each each dimension, for each grid / tile\n",
    "        bins = np.array([bins_per_dim] * number_of_grids)\n",
    "        # The offset is used to setup the overlap of grids\n",
    "        # offsets = np.array([[0.0, 0.0]]) is one grid starting from lows to highs \n",
    "        # offsets = np.array([[20.0, 20.0]]) is one grid starting from lows+[20.0, 20.0] to highs+[20.0, 20.0]\n",
    "\n",
    "        self.grids = []\n",
    "\n",
    "        for l, h, b, o in zip(lows, highs, bins, offsets):\n",
    "            grid = {}\n",
    "            grid['size']  = b\n",
    "            grid['low'] = l\n",
    "            grid['offset'] = o\n",
    "            grid['points'] = []\n",
    "            grid['step'] = []\n",
    "            for dim in range(len(b)):\n",
    "                points, step = np.linspace(l[dim], h[dim], b[dim]+1, endpoint=False, retstep=True)\n",
    "                points += o[dim]\n",
    "                grid['points'].append(points)\n",
    "                grid['step'].append(step)\n",
    "\n",
    "            grid['step'] = np.array(grid['step'])\n",
    "            grid['weights'] = np.zeros(grid['size'])\n",
    "            self.grids.append(grid)\n",
    "\n",
    "    # Get the sum of the weights for given continuous coordinates\n",
    "    def get_weight(self, sample):\n",
    "        encoded_sample = self.tile_encode(sample)\n",
    "        w = 0.0\n",
    "        for grid, (x,y) in zip(self.grids, encoded_sample):\n",
    "            w += grid['weights'][x,y]\n",
    "        return w\n",
    "    \n",
    "    # Set the weights for given continuous coordinates\n",
    "    def set_weight(self, sample, target):\n",
    "        encoded_sample = self.tile_encode(sample)\n",
    "        for grid, (x,y) in zip(self.grids, encoded_sample):\n",
    "            grid['weights'][x,y] = target/len(self.grids)\n",
    "\n",
    "    # Return the discrete coordinates from continuous coordinates for all grids\n",
    "    def tile_encode(self, sample):\n",
    "        encoded_sample = []\n",
    "        for grid in self.grids:\n",
    "            encoded_sample.append(self.discretize(sample, grid))\n",
    "        return encoded_sample    \n",
    "    \n",
    "    # Return the discrete coordinates from continuous coordinates for a given grid\n",
    "    def discretize(self, sample, grid):\n",
    "        sample = np.array(sample) - (grid['low'] + grid['offset'])\n",
    "        sample = np.maximum(sample, np.array([0]*len(grid['size'])))\n",
    "        index = sample // grid['step']\n",
    "        index = np.minimum(index, grid['size']-1)\n",
    "        return list(index.astype(int))\n",
    "\n",
    "    # Plot the different grids\n",
    "    def visualize_tilings(self):\n",
    "        \"\"\"Plot each tiling as a grid.\"\"\"\n",
    "        prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "        colors = prop_cycle.by_key()['color']\n",
    "        linestyles = ['-', '--', ':']\n",
    "        legend_lines = []\n",
    "        \n",
    "        for i, grid in enumerate(self.grids):\n",
    "            for x in grid['points'][0]:\n",
    "                l = plt.axvline(x=x, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)], label=i)\n",
    "            for y in grid['points'][1]:\n",
    "                l = plt.axhline(y=y, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)])\n",
    "            legend_lines.append(l)\n",
    "        plt.legend(legend_lines, [\"Tiling #{}\".format(t) for t in range(len(legend_lines))], facecolor='white', framealpha=0.9)\n",
    "        plt.title(\"Tilings\")\n",
    "\n",
    "\n",
    "# default setting for 3 tilings:\n",
    "offsets = np.array([[0.0, 0.0], [20.0, 20.0], [-20.0, 15.0]])\n",
    "tilings = grid_tilings(3, offsets)\n",
    "\n",
    "# example for one tiling\n",
    "# tilings = grid_tilings()\n",
    "\n",
    "plt.imshow(IMAGE[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "tilings.visualize_tilings()\n",
    "\n",
    "# Test with some sample values\n",
    "samples = np.random.rand(5, 2) * 264\n",
    "print(\"\\nSamples:\", samples, sep=\"\\n\")\n",
    "encoded_samples = [tilings.tile_encode(sample) for sample in samples]\n",
    "print(\"\\nIndexes of samples:\", *[s for s in encoded_samples], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following functions complete the definition of the environment. The agent's movements always go in the intended direction, but the distance travelled has a small random component. Besides by reaching the goal, the episode also terminates after TIME_LIMIT (200) steps; at that point, the agent gets a negative reward TIMEOUT_REWARD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gymnasium.Env):\n",
    "    metadata = {'render.modes': ['human', 'rgba_array']}\n",
    "    bx = np.array([AugmentedArea.WEST, AugmentedArea.WEST, AugmentedArea.EAST, AugmentedArea.EAST, AugmentedArea.WEST])\n",
    "    by = np.array([AugmentedArea.NORTH, AugmentedArea.SOUTH, AugmentedArea.SOUTH, AugmentedArea.NORTH, AugmentedArea.NORTH])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_actions = Actions\n",
    "        self.action_space = spaces.Discrete(len(self.num_actions))\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "        self.display = None\n",
    "        self.img, self.img_container = Environment._init_visual_area(IMAGE)\n",
    "        self.time = 0\n",
    "\n",
    "    def seed(self, seed=None) -> int:\n",
    "        np.random.seed(seed)\n",
    "        return seed\n",
    "\n",
    "    def step(self, action: Actions):\n",
    "        assert self.action_space.contains(action)\n",
    "        (x, y), was_invalid = self._validate_state(self._move(self.state, action))\n",
    "\n",
    "        self.state = (x, y)\n",
    "        reward = MOVE_REWARD if not was_invalid else INVALID_MOVE_REWARD\n",
    "        reward = TIMEOUT_REWARD if self.time >= TIME_LIMIT else reward\n",
    "        reward = self.time * reward\n",
    "        done = is_goal_reached(self.state) or self.time >= TIME_LIMIT\n",
    "        self.time += 1\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self, state: State = None) -> State:\n",
    "        self.state = self.starting_state() if not state else state\n",
    "        self.time = 0\n",
    "        return self.state\n",
    "\n",
    "    # returns the current environment situation\n",
    "    def render(self, mode='rgba_array'):\n",
    "        curr_img = np.array(self.img_container.get_array())\n",
    "        x, y = self.state\n",
    "        scaler = 4\n",
    "        w, e, n, s = x - scaler, x + scaler, y - scaler, y + scaler\n",
    "        curr_img[n:s, w:e, 0] = 255\n",
    "        curr_img[n:s, w:e, 1] = 0\n",
    "        curr_img[n:s, w:e, 2] = 255\n",
    "        curr_img[n:s, w:e, 3] = 255\n",
    "        cropped_img = curr_img  # Just for debugging purposes\n",
    "        if mode == 'rgba_array':\n",
    "            plt.close()\n",
    "            return cropped_img  # return RGB frame suitable for video\n",
    "        elif mode == 'human':\n",
    "            container = plt.imshow(curr_img)\n",
    "            ax = container.axes\n",
    "            ax.set_xlim(Boundary.WEST, Boundary.EAST, auto=None)\n",
    "            ax.set_ylim(Boundary.SOUTH, Boundary.NORTH, auto=None)\n",
    "            return container\n",
    "        else:\n",
    "            raise Exception(f\"Please specify either 'rgba_array' or 'human' as mode parameter!\")\n",
    "\n",
    "    # Return a randomly chosen non-terminal state as starting state\n",
    "    def starting_state(self) -> State:\n",
    "        while True:\n",
    "            state = (\n",
    "                np.random.randint(Boundary.WEST, Boundary.EAST + 1),\n",
    "                np.random.randint(Boundary.NORTH, Boundary.SOUTH + 1),\n",
    "            )\n",
    "            if not is_goal_reached(state):\n",
    "                return state\n",
    "\n",
    "    @staticmethod\n",
    "    def _move(state: State, action: Actions) -> State:\n",
    "        x, y = state\n",
    "        if action == Actions.NORTH:\n",
    "            y -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.SOUTH:\n",
    "            y += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.WEST:\n",
    "            x -= AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        elif action == Actions.EAST:\n",
    "            x += AVG_MOVEMENT_SIZE + np.random.randint(5) - 2\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_state(state: State) -> Tuple[State, bool]:\n",
    "        x, y = state\n",
    "        is_invalid = False\n",
    "        if y < Boundary.NORTH:\n",
    "            is_invalid = True\n",
    "            y = int(Boundary.NORTH)\n",
    "        if y > Boundary.SOUTH:\n",
    "            is_invalid = True\n",
    "            y = int(Boundary.SOUTH)\n",
    "        if x < Boundary.WEST:\n",
    "            is_invalid = True\n",
    "            x = int(Boundary.WEST)\n",
    "        if x > Boundary.EAST:\n",
    "            is_invalid = True\n",
    "            x = int(Boundary.EAST)\n",
    "        return (x, y), is_invalid\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_visual_area(img) -> np.ndarray:\n",
    "        x, y = img.shape\n",
    "        my_dpi = 80\n",
    "        fig = Figure(figsize=(y / my_dpi, x / my_dpi), dpi=my_dpi)\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        ax = fig.gca()\n",
    "\n",
    "        ax.plot(GOAL[0], GOAL[1], 'ro', linewidth=5)\n",
    "        ax.plot(Environment.bx, Environment.by, 'b-')\n",
    "        img_container = ax.imshow(img[:, :], cmap='gray', vmin=0, vmax=1.0)\n",
    "        ax.axis('off')\n",
    "        fig.tight_layout()\n",
    "        canvas.draw()  # draw the canvas, cache the renderer\n",
    "        s, (width, height) = canvas.print_to_buffer()\n",
    "        image = np.frombuffer(s, np.uint8).reshape((height, width, 4))\n",
    "        img_container.set_data(image)\n",
    "        plt.close()\n",
    "        return image, img_container\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement your agent for Semi-gradient SARSA and Q-Learning\n",
    "\n",
    "Next comes your part. We start with integrating the linear approximation function with Tabular RL methods. \n",
    "\n",
    "**2.1** The following class is responsible for the agent's behavior. The select_action function should implement the epsilon-greedy policy, and return an action chosen according to that policy. **Please fill in the missing codes in select_action function (1.5 points).** \n",
    "\n",
    " Remark: This is an abstract class.\n",
    " Hence, its sole purpose is creating subclasses from it, which is also the reason it cannot be instantiated.\n",
    " The following subsequent subclasses will provide a specific implementation for the methods that are missing here.\n",
    " Therefore, you can ignore the functions that are not implemented. This is just a common way to make sure that all subclasses behave similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float):\n",
    "        # set up the value of epsilon\n",
    "        self.alpha = alpha  # learning rate or step size\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.hist = []\n",
    "\n",
    "    # Choose action at state based on epsilon-greedy policy and valueFunction\n",
    "    def select_action(self, state: Tuple[int, int], use_greedy_strategy: bool = True) -> int:\n",
    "        values = np.array(self.values(state))  # shape (4,)\n",
    "        self.hist.append(values)\n",
    "        \n",
    "        # Pure greedy (used when evaluating): no exploration\n",
    "        if use_greedy_strategy:\n",
    "            epsilon = 0\n",
    "        else:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        # ε-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            return int(np.random.choice(len(values)))   # explores\n",
    "        \n",
    "        else:\n",
    "            max_value = np.max(values)\n",
    "            best_actions = np.where(values == max_value)[0]\n",
    "            return int(np.random.choice(best_actions)) \n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    @abstractmethod\n",
    "    def value(self, state: State, action: Actions) -> float:\n",
    "        pass\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    @abstractmethod\n",
    "    def values(self, state: State) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    # Set value for given state and action\n",
    "    @abstractmethod\n",
    "    def set_value(self, state: State, action: Actions):\n",
    "        pass\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy\n",
    "    @abstractmethod\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False) -> int:\n",
    "        return None\n",
    "\n",
    "    # Return estimated state value, based on the estimated action values\n",
    "    def state_value(self, state):\n",
    "        return np.max(self.values(state))\n",
    "\n",
    "    # Plot the state value estimates. Use a larger stride for lower resolution.\n",
    "    def plot_state_values(self, stride=1):\n",
    "        self.v = np.zeros(\n",
    "            ((Boundary.SOUTH - Boundary.NORTH + stride) // stride, (Boundary.EAST - Boundary.WEST + stride) // stride))\n",
    "        for j, x in enumerate(range(Boundary.WEST, Boundary.EAST + 1, stride)):\n",
    "            for i, y in enumerate(range(Boundary.NORTH, Boundary.SOUTH + 1, stride)):\n",
    "                self.v[i, j] = self.state_value((x, y))\n",
    "\n",
    "        plt.imshow(self.v)\n",
    "        plt.colorbar()\n",
    "        return plt.show()\n",
    "\n",
    "    def plot_q_values(self, skip=1):\n",
    "        return pd.DataFrame(self.hist[::skip]).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** The next classes are two agents using either episodic semi-gradient Q-Learning and episodic semi-gradient SARSA algorithm to estimate the value function. Both agents use the same linear function approximation method with tile coding. **Implement the `learn` function according to the update rule for the respective algorithm (1 point for each)**. \n",
    " \n",
    " REMARK: Both agents use the same tile coding. This method helps splitting the state-space into discrete chunks. Each chunk is associated with one weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles the function approximation, with several methods to query and update it. \n",
    "# A linear approximation function is used, making the computation much faster.\n",
    "class QAgent(Agent):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float, num_tilings: int = 3, bins_per_dim: tuple = (9, 9)):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "        # Use a tile coding\n",
    "        self.tilings = [grid_tilings(number_of_grids=num_tilings, bins_per_dim=bins_per_dim) for a in Actions]\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]\n",
    "    \n",
    "    # Set value for given state and action\n",
    "    def set_value(self, state: State, action: Actions, value: float):\n",
    "        self.tilings[action].set_weight(state, value)\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy: for Qlearning, the agent does not need to return the next selected action\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False):\n",
    "        # TO BE FILLED (1 point)\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent(Agent):\n",
    "    def __init__(self, alpha: float, gamma: float, epsilon: float, num_tilings: int = 3, bins_per_dim: tuple = (9, 9)):\n",
    "        super().__init__(alpha, gamma, epsilon)\n",
    "\n",
    "        # Use a tile coding\n",
    "        offsets = np.array([[0.0, 0.0], [20.0, 20.0], [-20.0, 15.0]])\n",
    "        self.tilings = [grid_tilings(number_of_grids=num_tilings, bins_per_dim=bins_per_dim, offsets=offsets) for a in Actions]\n",
    "\n",
    "    # Return estimated action value of given state and action\n",
    "    def value(self, state, action):\n",
    "        if is_goal_reached(state):\n",
    "            return 0.0\n",
    "        return self.tilings[action].get_weight(state)\n",
    "\n",
    "    # Return vector of estimated action values of given state, for each action\n",
    "    def values(self, state):\n",
    "        if is_goal_reached(state):\n",
    "            return np.zeros(4)\n",
    "        return [self.tilings[action].get_weight(state) for action in Actions]\n",
    "\n",
    "    # Set value for given state and action\n",
    "    def set_value(self, state: State, action: Actions, value: float):\n",
    "        self.tilings[action].set_weight(state, value)\n",
    "    \n",
    "    # learn with given state, action and target\n",
    "    # different between on-policy and off-policy: for SARSA, the agent needs to return the next selected action\n",
    "    # Sarsa update rule: Q(s,a)←Q(s,a)+α[r+γQ(s′,a′)−Q(s,a)]\n",
    "    def learn(self, state: State, action: Actions, next_state: State, reward: float, done: bool = False):\n",
    "        q_sa = self.value(state, action)\n",
    "\n",
    "        # On-policy so it must select the next action using epsilon-greedy\n",
    "        next_action = self.select_action(next_state, use_greedy_strategy=False)\n",
    "\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            q_snext_anext = self.value(next_state, next_action)\n",
    "            target = reward + self.gamma * q_snext_anext\n",
    "\n",
    "        # Semi-gradient update\n",
    "        new_value = q_sa + self.alpha * (target - q_sa)\n",
    "        self.set_value(state, action, new_value)\n",
    "\n",
    "        return next_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following function handles the interaction between agent and environment for a single episode. By passing the same value_function object in multiple calls to this function, the agent can learn from a sequence of such interactions.\n",
    "\n",
    " **Please fill in the missing parts (1 point).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env: Environment in which the agent is supposed to run \n",
    "# agent: agent to learn\n",
    "# initital state: Starting state for the environment\n",
    "# is_learning: should the value function be updated during the simulation?\n",
    "# is_rendering: should the run be recorded? (Takes some time to compute)\n",
    "\n",
    "\n",
    "def run_episode(env: Environment,\n",
    "                agent: Agent,\n",
    "                initial_state: State,\n",
    "                is_learning: bool = True,\n",
    "                is_rendering: bool = False) -> Tuple[State, float]:\n",
    "    # Initialize reward for episode\n",
    "    total_reward = 0.0\n",
    "    # Initialize the policy (if is_greedy=True then the agent follows its optimal policy, otherwise it will randomly select an action)\n",
    "    is_greedy = not is_learning\n",
    "    # Get initial action\n",
    "    current_state = initial_state\n",
    "    current_action = agent.select_action(initial_state, use_greedy_strategy=is_greedy)\n",
    "\n",
    "    # Track the rendering\n",
    "    animation_data = []\n",
    "    animation_data.append((env.render(), env.time, None, current_state, None, 0))\n",
    "    # Initialize variables\n",
    "    next_state = None\n",
    "    done = False\n",
    "    next_action = None\n",
    "    while not done:\n",
    "        # TO BE FILLED\n",
    "        next_state, reward, done, _ = ???\n",
    "\n",
    "        total_reward += reward\n",
    "        if is_rendering:\n",
    "            curr_img = env.render()\n",
    "            animation_data.append((curr_img, env.time, current_state, next_state, current_action, total_reward))\n",
    "        \n",
    "        # Execute the learning and update the state and action\n",
    "        # TO BE FILLED (1 point)\n",
    "        \n",
    "    agent.last_action = None\n",
    "    return current_state, total_reward, animation_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To help understand your agent, you can render the agent's performance by setting render to True and running the `run_epoch` function. \n",
    " \n",
    " There are some helper functions. They might help you implement the agent correctly. \n",
    " * `agent.plot_state_values` shows you how the agent values different states\n",
    " * `agent.plot_q_values` shows the q_values that the agent had over the course of his life time. (That could be a lot. There's a skip parameter to reduce the amount of data points)\n",
    "\n",
    " REMARK: Keep in mind, the following is just one example run. Don't expect the model to be fully trained after just one episode. The training part follows in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "epsilon = 0.0\n",
    "gamma = .7\n",
    "alpha = 1e-2\n",
    "agent = QAgent(alpha, gamma, epsilon)\n",
    "start_state = env.reset()\n",
    "print(start_state)\n",
    "end_state, total_reward, animation_data = run_episode(env, agent, start_state, is_learning=True, is_rendering=True)\n",
    "print(end_state)\n",
    "agent.plot_state_values()\n",
    "agent.plot_q_values(skip=1)\n",
    "render_epoch(animation_data, interval=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the simulation, play with parameters and analyse results\n",
    "\n",
    " Now it's time to train both algorithms/agents on the environment. \n",
    " \n",
    " In the simulations, please plot the measure of each algorithm's performance as a function of the episode (i.e. the sum of all immediate rewards received in each episode). You shall play with a few combinations of two parameters discount factor $\\gamma$ and stepsize $\\alpha$ (at least two variables for each parameter). During the experiments, keep $\\epsilon$ fixed at $0.01$. A reasonable starting point for $\\alpha$ is 1e-2. \n",
    " \n",
    " REMARK: You can save the parameters and update-to-date Q table of each agent, so that you can still test their performance later. (You can achieve this by keeping the object.)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Please submit your code as well as the plots presenting compariable performance of the different combinations of parameters for every algorithms (2 points)**. \n",
    "\n",
    "REMARK: For a decent comparison all agents should be plotted on the same axis bounds. Also, the plots may be hard to interpret because of the scales. Feel free to do your own smoothing or use the `smooth` function provided in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for running the simulation and ploting the sum of immediate reward in each episode.\n",
    "\n",
    "# Init the environment and run Q-Learning & SARSA\n",
    "\n",
    "# Save and plot the results\n",
    "\n",
    "# TO BE FILLED.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your plots here. (In case your code takes too long to run.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Please interpret the results/plots (2 points)**. \n",
    "\n",
    "Explain what the results say about the parameters alpha (learning rate) and gamma (decay-rate), and their effects on learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Let us think one step further by looking at the policies you have learned (1.5 points).** \n",
    "\n",
    "Please compare the performance of your learned optimal policies using another simulation. In this simulation, you shall have three agents (Qlearning_Agent, SARSA_agent and Random_agent), where all three agents always use their learned policy to behave and find the goal in the given map (make sure the agent does NOT learn anymore!). The initial policies of Qlearning_Agent and SARSA_agent are the optimal policies learned from the above experiments (based on the final estimated Q-values using Q-Learning and SARSA algorithm respectively). The inital policy of Random_Agent will select 4 actions randomly. \n",
    "\n",
    "Describe the performance of three agents by running each agent on the task and discuss the results (1.5 points). You can use the render function provided above (and other helper functions) observe the different behaviors of three agents. You could also use an appropriate plot to show the different performances. **There is no need to submit your codes for this question.**\n",
    "\n",
    "Some questions you shall think about and answer (in case there are no differences or no special things, just indicate what you observed):\n",
    "- How does the learned Q-Learning policy and SARSA policy perform differently? Does it show the difference between on-policy and off-policy methods?\n",
    "- What kind of strategy did the two RL agents learn similarly? How do they differ from the random policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4. Implement your agent for DQN\n",
    "---\n",
    "\n",
    "Now we want to implement an agent for a deep RL algorithm DQN, and compare it with the other two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4.1 DQN uses two deep neural networks for training. A policy and a target network. Here we're defining the network architecture. You can just use the network as provided by us. But feel welcome to play around with changing the number of nodes per layers, the activation functions or the layers themselves. However, whether you play with the network or not will not affect your grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided DQN Architecture\n",
    "# PyTorch Q-network skeleton\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**4.2 Replay Buffer (0.5 points)**\n",
    "\n",
    "In DQN, we use a replay buffer. The Replay Buffer stores and manages experiences (state, action, reward, next_state, done[has terminated]) so the agent can learn from them multiple times.\n",
    "\n",
    "You will implement two methods:\n",
    "  - push method to store an experience in the buffer\n",
    "  - sample method to return a batch of experiences, i.e., (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    # push a transition to the memory (buffer)\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Store the transition as a tuple\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # randomly sample a batch of consecutive experiences; please convert to numpy array for later training\n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        # Batch is a random transition tuple\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unpack Batch\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Return as np array\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32)\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "While DQN can learn from continuous features, in our target task, learning from just x and y coordinate makes DQN very data hungry. To decrease the training load, we use a Radial Basis Function to embed the x and y coordinates. A Radial Basis Function transforms a distance to a value between 0 and 1 (with 1 meaning the distance is 0 and 0 meaning the distance is infinity). We use 9 points sampled across the input space as references and transform a position (x,y) into a vector containing 9 values between 0 and 1 by using the Gaussian kernel to transform distances.\n",
    "\n",
    "In this way, we are enriching the feature and transforming a 2D input into a 9D input with continuous and smooth values. There is no need to implement this function. Please use this function to embed the states before feeding them to DQN. This enables DQN to learn faster with fewer data. \n",
    "\n",
    "REMARK: Using a Radial Basis Function to embed the 2D coordinates as a similar approach to using a CNN to embed an image. This will not be covered in the exams!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Define RBF Hyperparameters ---\n",
    "N_CENTERS_PER_DIM = 3 # This will create 3x3 = 9 centers\n",
    "RBF_STATE_DIM = N_CENTERS_PER_DIM * N_CENTERS_PER_DIM # New state_dim = 9\n",
    "\n",
    "X_RANGE = Boundary.EAST - Boundary.WEST\n",
    "Y_RANGE = Boundary.NORTH - Boundary.SOUTH\n",
    "\n",
    "# Create a 3x3 grid of centers\n",
    "x_centers = np.linspace(Boundary.WEST + X_RANGE/4, Boundary.EAST - X_RANGE/4, N_CENTERS_PER_DIM)\n",
    "y_centers = np.linspace(Boundary.SOUTH + Y_RANGE/4, Boundary.NORTH - Y_RANGE/4, N_CENTERS_PER_DIM)\n",
    "\n",
    "# Create a 9x2 array of (x, y) center coordinates\n",
    "RBF_CENTERS = np.array(np.meshgrid(x_centers, y_centers)).T.reshape(-1, 2)\n",
    "\n",
    "\n",
    "# Define the \"width\" of the gaussian kernels\n",
    "RBF_SIGMA = (X_RANGE / N_CENTERS_PER_DIM) * 0.5\n",
    "\n",
    "print(f\"Using {RBF_STATE_DIM} RBF features (centers) with sigma={RBF_SIGMA:.2f}\")\n",
    "\n",
    "# --- RBF embedding function ---\n",
    "def embed_state_rbf(state):\n",
    "    \"\"\"\n",
    "    Calculates the 'closeness' (Gaussian) to each RBF center.\n",
    "    \"\"\"\n",
    "    state_vec = np.array(state, dtype=np.float32) # [x, y]\n",
    "\n",
    "    # Calculate squared distance from state to centers\n",
    "    sq_distances = np.sum((state_vec - RBF_CENTERS)**2, axis=1)\n",
    "\n",
    "    # Apply the Gaussian (bell curve) formula\n",
    "    # exp(-dist^2 / (2 * sigma^2))\n",
    "    features = np.exp(-sq_distances / (2 * RBF_SIGMA**2))\n",
    "\n",
    "    return features.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**4.3: DQN** \n",
    "\n",
    "Now you are ready to define the DQN agent.\n",
    "Please implement the following methods (**2 points**):\n",
    "\n",
    "- select_action\n",
    "- learn\n",
    "- run_episode_dqn\n",
    "\n",
    "Note that you should use the embed_state_rbf() method to embed states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "    def __init__(self, state_dim, n_actions, gamma, epsilon_start, epsilon_end, epsilon_decay, learning_rate, buffer_size, batch_size, target_update_freq):\n",
    "        # Note: We pass learning_rate as 'alpha' and epsilon_start as 'epsilon'\n",
    "        # state_dim : dimension of the (embedded) input space\n",
    "        # n_action : number of actions the agent can take\n",
    "        # gamma: discount factor\n",
    "        # epsilon_end: we will decrease epsilon over time, epsilon_end is the minimal epsilon value\n",
    "        # epsilon_decay: how much we will decrease epsilon with each update\n",
    "        # batch_size: how many consecutive are use for one training update\n",
    "        # target_update_freq: the frequency with which the target net is updated\n",
    "        super().__init__(alpha=learning_rate, gamma=gamma, epsilon=epsilon_start)\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.policy_net = QNetwork(state_dim, n_actions).to(self.device)\n",
    "        self.target_net = QNetwork(state_dim, n_actions).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayBuffer(self.buffer_size)\n",
    "\n",
    "        self.steps_done = 0\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def __del__(self):\n",
    "        # Clean up CUDA memory if used\n",
    "        if self.device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def values(self, state):\n",
    "        # Gets Q-values for a state that is embedded.\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values_tensor = self.policy_net(state_tensor)\n",
    "            return q_values_tensor.cpu().numpy().flatten()\n",
    "\n",
    "    def value(self, state, action):\n",
    "        # Get Q-value for a specific state-action pair\n",
    "        q_values = self.values(state)\n",
    "        return q_values[action]\n",
    "\n",
    "\n",
    "    def set_value(self, state: State, action: Actions, value: float):\n",
    "        # This method is for tabular agents, not directly applicable to DQN\n",
    "        # We can't directly set a weight for a single state-action pair\n",
    "        pass\n",
    "\n",
    "    # Choose an action for the given state.\n",
    "    # Returns the choosen action\n",
    "    def select_action(self, state, use_greedy_strategy=False):\n",
    "        # TO BE FILLED.\n",
    "\n",
    "    # This function takes in a transition and saves it to the memory.\n",
    "    # Note that you can only learn, if the memory has at least as many saved experiences as the batch size.\n",
    "    # Think carefully about the roles of the target and the policy network and when to update which\n",
    "    # You should use torch.nn.MSELoss() to compute the loss, and the optimizer that is part of the agent (self.optimizer)\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "\n",
    "        # TO BE FILLED.\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def state_value(self, state):\n",
    "        \"\"\"\n",
    "        Returns the state-value V(s) = max_a Q(s,a)\n",
    "        Used by Agent.plot_state_values()\n",
    "        \"\"\"\n",
    "        q = self.values(state)\n",
    "        return float(np.max(q))\n",
    "\n",
    "    def plot_state_values(self, stride=1):\n",
    "        self.v = np.zeros(\n",
    "            ((Boundary.SOUTH - Boundary.NORTH + stride) // stride, (Boundary.EAST - Boundary.WEST + stride) // stride))\n",
    "        for j, x in enumerate(range(Boundary.WEST, Boundary.EAST + 1, stride)):\n",
    "            for i, y in enumerate(range(Boundary.NORTH, Boundary.SOUTH + 1, stride)):\n",
    "                self.v[i, j] = self.state_value(embed_state_rbf((x, y)))\n",
    "\n",
    "        plt.imshow(self.v)\n",
    "        plt.colorbar()\n",
    "        return plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is what you may use to train the agent for one episode. You need to set-up the environment. \n",
    "# Until the episode terminates the agent should interact with the environment. \n",
    "# is_learning and is_rendering indicate if the agent should also learn (update weights) and if the data for antimating the episode should be collected and returned. \n",
    "# In that case make sure to save env.render(), env.time, the previous state, the new state, the choosen action and the total reward for every timestep.\n",
    "\n",
    "def run_episode_dqn(env, agent, is_learning=True, is_rendering=False):\n",
    "    state = env.reset()\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    state = embed_state_rbf(state) # embed state\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    animation_data = []\n",
    "\n",
    "    if is_rendering:\n",
    "        animation_data.append((env.render(), env.time, None, state, None, 0))\n",
    "\n",
    "    while not done:\n",
    "        # TO BE FILLED.\n",
    "\n",
    "    return total_reward, animation_data # Return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**4.4 Run the simulation, play with parameters and analyse results (1 point)**\n",
    "\n",
    "Now it's time to train and evaluate DQN algorithm/agent on the environment. \n",
    "Similar with the other two algorithms, in the simulations, please plot the measure of DQN's performance. \n",
    "You shall play with two parameters: stepsize (learning rate) $\\alpha$ and update frequency of target network ($c$ in the slides), and find good values for them. During the experiments, you can stick with the values provided below for the remaining hyperparameters.\n",
    "\n",
    "There is no need to submit your code for this answer. Please provide the final plots of learning performance for DQN with different parameters.\n",
    "\n",
    "Tips: When training the DQN agent, you can use a warm-up stage to fill up the memory of this agent with experiences (following the random policy) to speed up the training process. You may also plot the state values to see how the agent learns over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_episodes_dqn = 4000\n",
    "gamma_dqn = 0.99\n",
    "epsilon_start_dqn = 1.0\n",
    "epsilon_end_dqn = 0.05\n",
    "epsilon_decay_dqn = (epsilon_start_dqn - epsilon_end_dqn) / (num_episodes_dqn * 0.9) # Decay over 90% of episodes\n",
    "\n",
    "buffer_size_dqn = 10000\n",
    "batch_size_dqn = 64\n",
    "\n",
    "num_seed_dqn = 42\n",
    "\n",
    "# To be played with, and fill in your final setting, please notice your below results shall follw these two parameters\n",
    "learning_rate_dqn = ?\n",
    "target_update_freq_dqn = ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add your final plots here and explain your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**4.5 Finally, let us compare DQN with Semi-gradient SARSA and Q-learning (1.5 points)**\n",
    "\n",
    "We require you to train all three algorithms together, compare their learning processes and explain the final learned policies.\n",
    "\n",
    "**You do not need to submit your code for this question**. Please provide one plot that shows the learning curves of all three algorithms, and then explain your observations. To understand the learned policy, please use the code below to visualize the learned policy for agents, explaining the learned policy and arguing whether it is optimal.\n",
    "\n",
    "Some questions you shall think about and answer (in case there are no differences or no special things, just indicate what you observed):\n",
    "- How do the learning procedures of the three algorithms differ?\n",
    "- What strategy does each RL agent appear to have learned? How is the final learned policy of the three algorithms different (or similar)? - When do we use different algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Put your plots for the learning procedure and final learned policies here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the codes you can use to visualize the learned policy. For each possible state, the greedy policy is visualized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_agent_policy(agent, stride=1, dqn_agent=True):\n",
    "\n",
    "    # Plot the agent's policy for each state in the grid.\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    # Prepare the grid\n",
    "    x_coords = range(60, 205, stride)\n",
    "    y_coords = range(60, 205, stride)\n",
    "\n",
    "    # Create arrays to store actions and colors\n",
    "    action_grid = np.zeros((len(y_coords), len(x_coords)), dtype=int)\n",
    "    color_grid = np.empty((len(y_coords), len(x_coords)), dtype=object)\n",
    "\n",
    "    # Map actions to colors\n",
    "    action_colors = {\n",
    "        Actions.NORTH: 'blue',    # NORTH = BLUE\n",
    "        Actions.SOUTH: 'orange',     # SOUTH = ORANGE\n",
    "        Actions.EAST: 'green',        # EAST = GREEN\n",
    "        Actions.WEST: 'purple'       # WEST = PURPLE\n",
    "    }\n",
    "\n",
    "    # For each state, get the greedy action\n",
    "    for i, y in enumerate(y_coords):\n",
    "        for j, x in enumerate(x_coords):\n",
    "            # Embed the state\n",
    "            if dqn_agent:\n",
    "                embedded_state = embed_state_rbf((x, y))\n",
    "            else:\n",
    "                embedded_state = (x,y)\n",
    "\n",
    "            # Get greedy action\n",
    "            action = agent.select_action(embedded_state, use_greedy_strategy=True)\n",
    "\n",
    "            action_grid[i, j] = action\n",
    "            color_grid[i, j] = action_colors[action]\n",
    "\n",
    "    # Create the plot\n",
    "    for i, y in enumerate(y_coords):\n",
    "        for j, x in enumerate(x_coords):\n",
    "            ax.add_patch(plt.Rectangle((x, y), stride, stride,\n",
    "                                     facecolor=color_grid[i, j],\n",
    "                                     edgecolor='gray',\n",
    "                                     linewidth=0.5))\n",
    "\n",
    "    # Set plot properties\n",
    "    ax.set_xlim(60, 204)\n",
    "    ax.set_ylim(204, 60)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "    ax.set_title('Agent Policy Visualization\\n(Blue=North, Orange=South, Green=East, Purple=West)')\n",
    "\n",
    "    # Create legend\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='blue', label='NORTH/UP'),\n",
    "        Patch(facecolor='orange', label='SOUTH/DOWN'),\n",
    "        Patch(facecolor='green', label='EAST/RIGHT'),\n",
    "        Patch(facecolor='purple', label='WEST/LEFT')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    return plt.show()\n",
    "\n",
    "plot_agent_policy(agent, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Before handing in, make sure that the code you hand in work, and that all plots are shown. \n",
    "\n",
    "Again, please name this file according to your last names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
